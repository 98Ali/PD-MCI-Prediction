{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGB1qAPvaoif"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Here we are trying to make a binary classification between those whom MoCA score worthens or it does not based on just their year 0 and 1 DatScan\n",
        "ðŸ¥‘"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLFVSM5laoig"
      },
      "source": [
        "# **Setup**\n",
        "\n",
        "1. **Import libraries**\n",
        "\n",
        "2. **Connect to google drive**\n",
        "\n",
        "3. **Initiate GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XkMqvctaoig"
      },
      "outputs": [],
      "source": [
        "!pip install classification-models-3D\n",
        "!pip install keras_applications\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from google.colab import drive\n",
        "import nibabel as nib\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "import glob as glob\n",
        "from os.path import exists\n",
        "from IPython.display import clear_output \n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from classification_models_3D.tfkeras import Classifiers\n",
        "\n",
        "#Connect Google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Initialize GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else: print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = glob.glob(\"/content/drive/MyDrive/datscans/*.nii\")\n",
        "for filee in files:\n",
        "  img = nib.load(filee)\n",
        "  if str(img.shape)==\"(91, 109, 91, 1)\":\n",
        "   print(filee.split(\"/\")[-1]+\" \"+str(img.shape))\n",
        "  else:\n",
        "    print(filee.split(\"/\")[-1]+\" \"+\"NOT CORRECT SHAPE\")\n",
        "    os.remove(filee)\n"
      ],
      "metadata": {
        "id": "rWqF6LFCnwQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dicom_file(filepath):\n",
        "    dicom = pydicom.dcmread(filepath)\n",
        "    return dicom"
      ],
      "metadata": {
        "id": "p9SPjlHn-Nam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_40c9CdQTlR"
      },
      "outputs": [],
      "source": [
        "def read_file(filepath):\n",
        "    \"\"\"Read and load volume\"\"\"\n",
        "    scan = read_dicom_file(filepath)\n",
        "    scan = scan.pixel_array\n",
        "    return scan\n",
        "\n",
        "def crop (image,x1,x2,y1,y2,z1,z2):\n",
        "    \"\"\"Crop image\"\"\"\n",
        "    if (x2-x1)%2 != 0: x1 = x1 - 1\n",
        "    if (y2-y1)%2 != 0: y1 = y1 - 1\n",
        "    if (z2-z1)%2 != 0: z1 = z1 - 1\n",
        "\n",
        "    xdiff = round((22 - x2 + x1)/2)\n",
        "    ydiff = round((28 - y2 + y1)/2)\n",
        "    zdiff = round((40 - z2 + z1)/2)\n",
        "\n",
        "    #volume = image[y1 - ydiff : y2 + ydiff,x1 - xdiff : x2 + xdiff, z1 - zdiff : z2 + zdiff]\n",
        "    volume = image[:,:, z1 - zdiff : z2 + zdiff]\n",
        "    return volume\n",
        "\n",
        "def normalise_zero_one(image):  \n",
        "    \"\"\"Image normalisation. Normalises image to fit [0, 1] range.\"\"\"\n",
        "    image = image.astype(np.float32)\n",
        "    minimum = np.min(image)\n",
        "    maximum = np.max(image)\n",
        "    if maximum > minimum:\n",
        "        ret = (image - minimum) / (maximum - minimum)\n",
        "    else:\n",
        "        ret = image * 0.\n",
        "    return ret\n",
        "\n",
        "\n",
        "def process_scan(dim_path):\n",
        "    \"\"\"Read and resize volume\"\"\"\n",
        "    volume = read_file(dim_path.get(\"path\"))\n",
        "    volume = crop(volume,dim_path.get(\"x1\"),dim_path.get(\"x2\"),dim_path.get(\"y1\"),dim_path.get(\"y2\"),dim_path.get(\"z1\"),dim_path.get(\"z2\"))\n",
        "    volume = normalise_zero_one(volume)\n",
        "    return volume\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paths = glob.glob('/content/drive/MyDrive/P202204050001/Codes and datasets/Datasets/preprocessed_dataset/year0/Segmented_SC/DICOM_SegmentedImages/*/image_Whole3D.dcm')\n",
        "len(paths)"
      ],
      "metadata": {
        "id": "0zIesRAZP2ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom\n",
        "import pydicom"
      ],
      "metadata": {
        "id": "CsqfwesbOo7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFxktHPxm9NJ"
      },
      "outputs": [],
      "source": [
        "counter =  0\n",
        "xdiffs = []\n",
        "ydiffs = []\n",
        "zdiffs = []\n",
        "x1s =[]\n",
        "y1s =[]\n",
        "z1s =[]\n",
        "x2s =[]\n",
        "y2s =[]\n",
        "z2s =[]\n",
        "my_dicts = []\n",
        "while counter < len(paths):\n",
        "  print(round(100*counter / len(paths)),\"% proccesing\")\n",
        "  dicom = read_dicom_file(paths[counter])\n",
        "  arr = dicom.pixel_array\n",
        "  clear_output()\n",
        "\n",
        "  a = 0\n",
        "  while a < arr.shape[0]:\n",
        "    if 255 in arr[a,:,:]:\n",
        "      x1 = a\n",
        "      x1s.append(x1)\n",
        "      break;\n",
        "    a+=1\n",
        "\n",
        "  la = arr.shape[0]\n",
        "  while la > 0:\n",
        "    if 255 in arr[la-1,:,:]:\n",
        "      x2 = la\n",
        "      x2s.append(x2)\n",
        "      break;\n",
        "    la-=1\n",
        "\n",
        "  xdiff = x2-x1\n",
        "  xdiffs.append(abs(xdiff))\n",
        "\n",
        "  b = 0\n",
        "  while b < arr.shape[1]:\n",
        "    if 255 in arr[:,b,:]:\n",
        "      y1 = b\n",
        "      y1s.append(y1)\n",
        "      break;\n",
        "    b+=1\n",
        "\n",
        "  lb = arr.shape[1]\n",
        "  while lb > 0:\n",
        "    if 255 in arr[:,lb-1,:]:\n",
        "      y2 = lb\n",
        "      y2s.append(y2)\n",
        "      break;\n",
        "    lb-=1\n",
        "\n",
        "  ydiff = y2-y1\n",
        "  ydiffs.append(abs(ydiff))\n",
        "\n",
        "  c = 0\n",
        "  while c < arr.shape[2]:\n",
        "    if 255 in arr[:,:,c]:\n",
        "      z1 = c\n",
        "      z1s.append(z1)\n",
        "      break;\n",
        "    c+=1\n",
        "\n",
        "  lc = arr.shape[2]\n",
        "  while lc > 0:\n",
        "    if 255 in arr[:,:,lc-1]:\n",
        "      z2 = lc\n",
        "      z2s.append(z2)\n",
        "      break;\n",
        "    lc-=1\n",
        "\n",
        "  zdiff = z2-z1\n",
        "  zdiffs.append(abs(zdiff))\n",
        "  my_dict = {}\n",
        "  my_dict['z1'] = z1\n",
        "  my_dict['x1'] = x1\n",
        "  my_dict['y1'] = y1\n",
        "  my_dict['z2'] = z2\n",
        "  my_dict['x2'] = x2\n",
        "  my_dict['y2'] = y2\n",
        "  my_dict['path'] = paths[counter]\n",
        "  my_dicts.append(my_dict)\n",
        "  counter += 1\n",
        "\n",
        "print(len(my_dicts))\n",
        "print(np.amin(z1s),\" : z1\")\n",
        "print(np.amin(y1s),\" : y1\")\n",
        "print(np.amin(x1s),\" : x1\")\n",
        "\n",
        "print(np.amax(z2s),\" : z2\")\n",
        "print(np.amax(x2s),\" : x2\")\n",
        "print(np.amax(y2s),\" : y2\")\n",
        "\n",
        "print(np.amax(xdiffs),\" : x\")\n",
        "print(np.amax(ydiffs),\" : y\")\n",
        "print(np.amax(zdiffs),\" : z\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5PXryKNaoij"
      },
      "source": [
        "# **Loading data and preprocessing**\n",
        "\n",
        "Preprocessing algorithm is inherited from diffrent papers\n",
        "\n",
        "\n",
        "* **Input size** :\n",
        "\n",
        "  Diffrent ROI sizes have been used in previous studies.\n",
        "\n",
        "  1.   79 x 95 x 69 -> [Paper](https://linkinghub.elsevier.com/retrieve/pii/S0895611120301051)\n",
        "  2.   91 x 109 x 91 -> Most papers\n",
        "  3.   32 X 48 X 16 -> [Paper](https://jnm.snmjournals.org/content/59/supplement_1/29)\n",
        "  \n",
        "  But in this study entire images are taken into consideration, and the network is trained accordingly. This, in turn, allows the network to be more robust and efficient in classifying PD patients from healthy controls patients because classification is performed based on the whole SPECT image of the brain. as suggested in [this](https://pubmed.ncbi.nlm.nih.gov/33279760/) study.â—Ž *and we have further remove 16 slices from the bottom of axial slices that mostly were damaged and not rendered correctly after reconstruction. then we removed a total of 18 slices from both sagital axis ends that contain no data and it helps the network to manage ram better.*â—Ž\n",
        "\n",
        "\n",
        "* **Normalization** :\n",
        "\n",
        "  Another preproccecing method that we are applying here is intensity normalization which is used in [this](https://arxiv.org/abs/2008.00238) paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg_dVJrbaoil"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def read_nifti_file(filepath):\n",
        "#     \"\"\"Read and load volume\"\"\"\n",
        "#     # Read file\n",
        "#     scan = nib.load(filepath)\n",
        "#     # Get raw data\n",
        "#     scan = scan.get_fdata()\n",
        "#     return scan\n",
        " \n",
        "# def normalise_zero_one(image):\n",
        "  \n",
        "#     \"\"\"Image normalisation. Normalises image to fit [0, 1] range.\"\"\"\n",
        "#     image = image.astype(np.float32)\n",
        "#     minimum = np.min(image)\n",
        "#     maximum = np.max(image)\n",
        "#     if maximum > minimum:\n",
        "#         ret = (image - minimum) / (maximum - minimum)\n",
        "#     else:\n",
        "#         ret = image * 0.\n",
        "#     return ret\n",
        "\n",
        "\n",
        "# def process_scan(path):\n",
        "#     \"\"\"Read and resize volume\"\"\"\n",
        "#     # Read scan\n",
        "#     volume = read_nifti_file(path)\n",
        "#     # Resize width, height and depth\n",
        "#     # volume = volume[0:91,9:100,16:91]\n",
        "#     # Normalize\n",
        "#     volume = normalise_zero_one(volume)\n",
        "  \n",
        "#     return volume\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz5oSs2Raoin"
      },
      "source": [
        "# **Importing Nifti files from tables**\n",
        "Let's read the paths of the DAT scans from the directories.\n",
        "And do the main preprocessing on both worsened and not not worthened cognitive impairment patients.\n",
        "We also read feature from the coresponding table.\n",
        "predictor features are chosen from [this](https://link.springer.com/article/10.1007/s00415-020-09757-9) study.\n",
        "It includes:\n",
        "1. Age at onset\n",
        "2. APOE\n",
        "3. RBD\n",
        "4. Hallucination\n",
        "5. PIGD\n",
        "\n",
        "in order to cover all possible cognitive impairment scales we use three diffrent database in all which we calculated year n and n+5 cognitive scores. those three methods are listed below and are inherited from [this](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0175674) paper.\n",
        "\n",
        "1. MoCA cut-off for PD of <26  ---> moca\n",
        "2. Using the detailed cognitive battery, cognitive impairment was defined as at least two test scores >1.5 standard deviations below the standardized mean score, a level of impairment within the recommend range (>1.0â€“2.0) of standard deviations below the mean to support a PD-MCI diagnosis[19]. Single scores were generated for each test, except for the HVLT-T, for which two scores were used ---> neuro\n",
        "3. The site investigatorâ€™s clinical diagnosis of cognitive impairment (PD-MCI or PDD) versus no cognitive impairment was made annually. Each site investigator was provided an instruction sheet that outlined how to assess cognitive decline, functional impairment, and general interpretation of cognitive tests to make a diagnosis of PD-MCI or PDD ---> cog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDLDHU4LG3YX"
      },
      "outputs": [],
      "source": [
        "# labels_list = []\n",
        "# import pydicom._storage_sopclass_uids\n",
        "# from pydicom import dcmread\n",
        "\n",
        "# scans_list = []\n",
        "# excel_path = \"/content/drive/MyDrive/P202204050001/Codes and datasets/Datasets/arman_dat_zero_moca_four.xlsx\"\n",
        "# moca_table = pd.read_excel(excel_path)\n",
        "# for dim_path in my_dicts:\n",
        "#     if 'Segmented_SC_orginal' not in dim_path[\"path\"]:\n",
        "#       newpath = dim_path[\"path\"].replace('Segmented_SC','Segmented_SC_orginal').replace('image_Whole3D.dcm','data3D.dcm')  \n",
        "#       dim_path.update({\"path\": newpath})\n",
        "#       print(dim_path[\"path\"])\n",
        "\n",
        "# for i, row in moca_table.iterrows():\n",
        "#     passs = []\n",
        "#     for my_path in my_dicts:\n",
        "#       if str(int(row.patno)) == my_path['path'].split('/')[-2]:\n",
        "#         print(\"image : \"+ str(row.patno)+\" At \"+str(round(i/len(moca_table)*100))+ \" % of preprocessing.\")\n",
        "#         procesed = process_scan(dim_path)\n",
        "#         labels_list.append(int(row.moca_class))\n",
        "#         scans_list.append(procesed)\n",
        "\n",
        "# scans = np.asarray(scans_list)\n",
        "# labels = np.asarray(labels_list)\n",
        "\n",
        "# print(\"Scans: \" + str(len(scans)))\n",
        "# print(\"labels: \" + str(len(labels)))\n",
        "\n",
        "# print(\"Images dimentions are: \" +str(scans[0].shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(scans.shape[1]):\n",
        "#   print(\"Original image : \")\n",
        "#   plt.imshow(scans[10][i, :, :], cmap=\"gist_gray\")\n",
        "#   plt.show()"
      ],
      "metadata": {
        "id": "NB__SDTYGMa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0ZNYIWqaoio"
      },
      "outputs": [],
      "source": [
        "feature = \"\"\n",
        "cognitive_scale = \"cog\"\n",
        "\n",
        "worsened_scans_list = []\n",
        "worsened_labels_list = []\n",
        "\n",
        "worsened_feature_list =[]\n",
        "not_worsened_feature_list =[]\n",
        "\n",
        "not_worsened_paths = []\n",
        "worsened_paths = [] \n",
        "\n",
        "not_worsened_scans_list = []\n",
        "not_worsened_labels_list = []\n",
        "\n",
        "cog_dat_table = pd.read_csv(\"/content/drive/MyDrive/progress_visualizer-master/\"+cognitive_scale+\n",
        "                            \"_dat_feature/main_tables/\"+cognitive_scale+\"_dat_\"+feature+\"main.csv\")\n",
        "for dim_path in my_dicts:\n",
        "    if 'Segmented_SC_orginal' not in dim_path[\"path\"]:\n",
        "      newpath = dim_path[\"path\"].replace('Segmented_SC','Segmented_SC_orginal').replace('image_Whole3D.dcm','data3D.dcm')  \n",
        "      dim_path.update({\"path\": newpath})\n",
        "      print(dim_path[\"path\"])\n",
        "      \n",
        "for i, row in cog_dat_table.iterrows():\n",
        "  passs = []\n",
        "  for my_path in my_dicts:\n",
        "    if str(int(row.PATNO)) == my_path['path'].split('/')[-2]:\n",
        "      path = \"/content/drive/MyDrive/year_zero_and_one_datscans/I\"+str(row.IMGID)+\".nii\"\n",
        "      #clear_output()\n",
        "      #print(\"image : \"+ str(row.IMGID)+\" At \"+str(round(i/len(cog_dat_table)*100))+ \" % of preprocessing.\")\n",
        "  \n",
        "      if row.CONUMDIFF < 0:\n",
        "        #print(\"-1\")\n",
        "        if exists(path):\n",
        "          procesed = process_scan(my_path)\n",
        "          print(procesed.shape)\n",
        "          worsened_scans_list.append(procesed)\n",
        "          worsened_paths.append(path)\n",
        "          if feature != \"\":\n",
        "            worsened_feature_list.append(row.FEATURE)\n",
        "          worsened_labels_list.append(0)\n",
        "      else:\n",
        "        #print(\"+1\")\n",
        "        if exists(path):\n",
        "          procesed = process_scan(my_path)\n",
        "          print(procesed.shape)\n",
        "          not_worsened_scans_list.append(procesed)\n",
        "          not_worsened_paths.append(path)\n",
        "          if feature != \"\":\n",
        "            not_worsened_feature_list.append(row.FEATURE)\n",
        "          not_worsened_labels_list.append(1)\n",
        "\n",
        "if feature != \"\":\n",
        "  worsened_feature = np.asarray(worsened_feature_list)\n",
        "\n",
        "worsened_scans = np.asarray(worsened_scans_list)\n",
        "worsened_labels = np.asarray(worsened_labels_list)\n",
        "\n",
        "if feature != \"\":\n",
        "  not_worsened_feature = np.asarray(not_worsened_feature_list)\n",
        "\n",
        "not_worsened_scans = np.asarray(not_worsened_scans_list)\n",
        "not_worsened_labels = np.asarray(not_worsened_labels_list) \n",
        "\n",
        "\n",
        "print(\"Worsened Scans: \" + str(len(worsened_scans)))\n",
        "print(\"Not Worsened Scans: \" + str(len(not_worsened_scans)))\n",
        "print(\"Images dimentions are: \" +str(not_worsened_scans[0].shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(worsened_scans.shape[1]):\n",
        "  print(\"Original image : \")\n",
        "  plt.imshow(worsened_scans[10][i, :, :], cmap=\"gist_gray\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "nCr4StJztkqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCV9vETZaoip"
      },
      "source": [
        "# **Build train and validation datasets**\n",
        "for model we need two input which are image and feature and and an input which is label in here. we also devide data to pre-augmneted train part which contains 2/3 of whole dataset and validation part which is 1/3 of the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvM0yIi5aoip"
      },
      "outputs": [],
      "source": [
        "image_train = np.concatenate((not_worsened_scans[:round(len(not_worsened_scans)-(len(not_worsened_scans)/2))],\n",
        "                          worsened_scans[:round(len(worsened_scans)-(len(worsened_scans)/2))]), axis=0)\n",
        "if feature != \"\":\n",
        "  feature_train = np.concatenate((not_worsened_feature[:round(len(not_worsened_feature)-(len(not_worsened_feature)/2))],\n",
        "                          worsened_feature[:round(len(worsened_feature)-(len(worsened_feature)/2))]), axis=0)\n",
        "\n",
        "label_train = np.concatenate((not_worsened_labels[:round(len(not_worsened_labels)-(len(not_worsened_labels)/2))],\n",
        "                          worsened_labels[:round(len(worsened_labels)-(len(worsened_labels)/2))]), axis=0)\n",
        "\n",
        "image_val = np.concatenate((not_worsened_scans[round(len(not_worsened_scans)-(len(not_worsened_scans)/2)):],\n",
        "                          worsened_scans[round(len(worsened_scans)-(len(worsened_scans)/2)):]), axis=0)\n",
        "if feature != \"\":\n",
        "  feature_val = np.concatenate((not_worsened_feature[round(len(not_worsened_feature)-(len(not_worsened_feature)/2)):],\n",
        "                          worsened_feature[round(len(worsened_feature)-(len(worsened_feature)/2)):]), axis=0)\n",
        "\n",
        "label_val = np.concatenate((not_worsened_labels[round(len(not_worsened_labels)-(len(not_worsened_labels)/2)):],\n",
        "                          worsened_labels[round(len(worsened_labels)-(len(worsened_labels)/2)):]), axis=0)\n",
        "\n",
        "if feature != \"\":\n",
        "  print(\"Train size : \"+str(image_train.shape[0])+\" \"+ str(feature_train.shape[0])+\" \"+str(label_train.shape[0]))\n",
        "  print(\"Validation size : \"+str(image_val.shape[0])+\" \"+ str(feature_val.shape[0])+\" \"+str(label_val.shape[0]))\n",
        "else:\n",
        "  print(\"Train size : \"+str(image_train.shape[0])+\" \"+str(label_train.shape[0]))\n",
        "  print(\"Validation size : \"+str(image_val.shape[0])+\" \"+str(label_val.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD8FWSHPI9jB"
      },
      "source": [
        "# **Data Augmentation**\n",
        "we augmented iamges just with flipping images left to write as suggested in most papers such as [this](https://pubmed.ncbi.nlm.nih.gov/33279760/) one. in order to maintain dataset size and labels we then double the size of labels and features with no further augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMM3b85e3yhd"
      },
      "outputs": [],
      "source": [
        "lrflip_augmented_images_list = []\n",
        "udflip_augmented_images_list = []\n",
        "\n",
        "for image in image_train:\n",
        "  lrflip_augmented_image = np.fliplr(image)\n",
        "  lrflip_augmented_images_list.append(lrflip_augmented_image)\n",
        "  udflip_augmented_image = np.flipud(image)\n",
        "  udflip_augmented_images_list.append(udflip_augmented_image)\n",
        "\n",
        "\n",
        "print(\"Original image : \")\n",
        "plt.imshow(np.squeeze(image_train[2][10, :, :]), cmap=\"magma\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Augmented images : \")\n",
        "udflip_augmented_images = np.asarray(udflip_augmented_images_list)\n",
        "lrflip_augmented_images = np.asarray(lrflip_augmented_images_list)\n",
        "\n",
        "plt.imshow(np.squeeze(lrflip_augmented_images[2][10, :, :]), cmap=\"magma\")\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(np.squeeze(udflip_augmented_images[2][10, :, :]), cmap=\"magma\")\n",
        "plt.show()\n",
        "\n",
        "if feature != \"\":\n",
        "  feature_train = np.concatenate((feature_train,feature_train,feature_train), axis=0)\n",
        "\n",
        "label_train = np.concatenate((label_train,label_train), axis=0)\n",
        "image_train = np.concatenate((image_train,lrflip_augmented_images), axis=0)\n",
        "\n",
        "\n",
        "if feature != \"\":\n",
        "  print(\"Train size : \"+str(image_train.shape[0])+\" \"+ str(feature_train.shape[0])+\" \"+str(label_train.shape[0]))\n",
        "  print(\"Validation size : \"+str(image_val.shape[0])+\" \"+ str(feature_val.shape[0])+\" \"+str(label_val.shape[0]))\n",
        "else:\n",
        "  print(\"Train size : \"+str(image_train.shape[0])+\" \"+str(label_train.shape[0]))\n",
        "  print(\"Validation size : \"+str(image_val.shape[0])+\" \"+str(label_val.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPg6J5oa2hNg"
      },
      "source": [
        "#**Our own 3D image classification model**\n",
        "\n",
        "Inspired from [this](https://10.1007/978-3-030-59354-4_15) paper, we design a 17 layer 3D CNN which comprises four 3D con-volutional (CONV) layers with two layers consisting of 64 filters followed by 128and 256 filters all with a kernel size of 3 Ã— 3 Ã— 3. Each CONV layer is followed bya max-pooling (MAXPOOL) layer with a stride of 2 and ReLU activation whichends with batch normalization (BN) layer. Essentially, our feature extractionblock consists of four CONV-MAXPOOL-BN modules. The final output fromthe feature extraction block is flattened and passed to a fully connected layerwith 512 neurons. We use an effective dropout rate of 60% similar to [31]. Due toa coding error, we implement this using two dropout layers [34]. The output isthen carried to a dense layer of 2 neurons with softmax activation for the binaryclassification problem. The network architecture is shown in Fig. 2.We consider keeping the network relatively simple to avoid overparameter-ization [32] problems with only 10,658,498 learnable parameters. This is alsomotivated by the fewer number of training samples and the memory challengesassociated with it. if you have limited data, then itâ€™s likely that you will also have to limit the complexity of the ML models you use, since models with many parameters, like deep neural networks, can easily overfit small data sets. Either way, itâ€™s important to identify this issue early on, and come up with a suitable (and defensible) strategy to mitigate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22H9OtHF2ffY"
      },
      "outputs": [],
      "source": [
        "def get_own_model(width, height, depth):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=16, kernel_size=3, activation=\"relu\",padding=\"same\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    own_output = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "   \n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, own_output, name=\"3dcnn\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxXnrfTVznN8"
      },
      "outputs": [],
      "source": [
        "def get_own_standalone_model(width, height, depth):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=16, kernel_size=3, activation=\"relu\",padding=\"same\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "   \n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHQk_VE7BLgz"
      },
      "outputs": [],
      "source": [
        "def get_rahmim_model(width, height, depth):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=16, kernel_size=7,strides=4, activation=\"relu\",padding=\"same\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=3,strides=2)(x)\n",
        "    x = layers.Conv3D(filters=32, kernel_size=5,strides=1, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=3,strides=2)(x)\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3,strides=1, activation=\"relu\",padding=\"same\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=3,strides=2)(x)\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    rahmim_output = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "   \n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, rahmim_output, name=\"3dcnn\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIEgIg4xK11n"
      },
      "source": [
        "#**3D Image branch part**\n",
        "We used two diffrent CNN architecture without their pretrained weight for our model as they were previously shown to have a good performance in 2D image classification.\n",
        "1. VGG16\n",
        "2. ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80nUxJAxC7gq"
      },
      "outputs": [],
      "source": [
        "from classification_models_3D.models.resnext import ResNeXt50\n",
        "from classification_models_3D.models.vgg16 import VGG16\n",
        "from classification_models_3D.models.inception_v3 import InceptionV3\n",
        "\n",
        "model_name = 'rahmim'\n",
        "\n",
        "if model_name == 'resnet50':\n",
        "  ResNet50, preprocess_input = Classifiers.get('resnet50')\n",
        "  image_model = ResNet50(input_shape=(22, 28, 40, 1),weights=None)\n",
        "  \n",
        "elif model_name == 'inceptionv3':\n",
        "  InceptionV3, preprocess_input = Classifiers.get('inceptionv3')\n",
        "  image_model = InceptionV3(input_shape=(91, 91, 75, 1),weights=None)\n",
        "\n",
        "elif model_name == 'own':\n",
        "  image_model = get_own_model(width=91, height=109, depth=91)\n",
        "\n",
        "elif model_name == 'rahmim':\n",
        "  image_model = get_rahmim_model(width=22, height=28, depth=40)\n",
        "\n",
        "elif model_name == \"own_std\":\n",
        "  image_model = get_own_standalone_model(width=91, height=109, depth=91)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qobXz5pgQuFK"
      },
      "source": [
        "#**Feature branch model part â­•ï¸**\n",
        "here we make a simple 4 layer model to train on feature numerical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8haOrFiax_V"
      },
      "outputs": [],
      "source": [
        "feature_model = keras.models.Sequential()\n",
        "feature_model.add(layers.Dense(16, activation='relu', input_shape=(1,)))\n",
        "feature_model.add(layers.Dense(64, activation='relu'))\n",
        "feature_model.add(layers.Dense(256, activation='relu'))\n",
        "feature_model.add(layers.Dense(2048, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6keYXRRTWS"
      },
      "source": [
        "#**Concatenation â­•ï¸**\n",
        "Concatenate both models and add some final layers at the end. and plot the final model scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhOp49DZbxjj"
      },
      "outputs": [],
      "source": [
        "combinedInput = layers.concatenate([image_model.output, feature_model.output])\n",
        "\n",
        "x = layers.Dense(units=512, activation=\"relu\")(combinedInput)\n",
        "x = layers.Dense(units=256, activation=\"relu\")(x)\n",
        "x = layers.Dense(units=128, activation=\"relu\")(x)\n",
        "x = layers.Dense(units=64, activation=\"relu\")(x)\n",
        "x = layers.Dense(units=16, activation=\"relu\")(x)\n",
        "x = layers.Dense(units=4, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "combined_output = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "if feature != \"\":\n",
        "  final_model = keras.Model(inputs=[image_model.input, feature_model.input], outputs=combined_output.output)\n",
        "else: \n",
        "  final_model = keras.Model(inputs=image_model.input, outputs=image_model.output)\n",
        "\n",
        "keras.utils.plot_model(final_model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDZ7RR8eaoiq"
      },
      "source": [
        "#**Running model â­•ï¸**\n",
        "Compiling and running model\n",
        "with Adam Optimizer and learning rate of 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s7uOHDI36qc"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 4\n",
        "!pip install tensorflow-addons\n",
        "import datetime\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', patience=40)\n",
        "mc = keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n",
        "tb = keras.callbacks.TensorBoard(log_dir='./logs_v2', write_images=True, write_graph=True)\n",
        "logdir = os.path.join(\"logs2\", datetime.datetime.now().strftime(\"%Y/%m/%d-%H:%M:%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "final_model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              metrics=['accuracy','percision' 'AUC'])\n",
        "if feature != \"\":\n",
        "  history = final_model.fit([image_train, feature_train],label_train,batch_size=4,\n",
        "                 epochs=N_EPOCHS, validation_data=([image_val, feature_val],label_val), callbacks=[tensorboard_callback, es, mc])\n",
        "else:\n",
        "  history = final_model.fit(image_train,label_train,batch_size=4,\n",
        "                 epochs=N_EPOCHS, validation_data=(image_val,label_val), callbacks=[tensorboard_callback, es, mc])\n",
        "print(history)\n",
        "model_json = final_model.to_json()\n",
        "with open(\"/content/drive/MyDrive/progress_visualizer-master/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE6SI1hATHy5"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}